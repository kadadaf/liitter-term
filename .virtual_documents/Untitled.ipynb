# 导入必要的库
import torch
from torch import nn
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertModel
from sklearn.metrics import accuracy_score
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained("nghuyong/ernie-3.0-base-zh")
model = BertModel.from_pretrained("nghuyong/ernie-3.0-base-zh")


# help(model)





#定义超参数
batch_size = 32
num_epochs = 10
learning_rate = 2e-5
num_classes = 16 
# 根据你的数据集修改类别数
# 定义分类器，可以根据需要修改
class Classifier(nn.Module):
    def __init__(self, model, num_classes):
        super().__init__()
        self.model = model
        self.dropout = nn.Dropout(0.1)
        self.linear = nn.Linear(model.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.model(input_ids, attention_mask)
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output)
        logits = self.linear(pooled_output)
        return logits

# 实例化分类器
classifier = Classifier(model, num_classes)

# 定义优化器和损失函数，可以根据需要修改
optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()


# 定义数据集，可以根据你的数据格式修改
class TextDataset(torch.utils.data.Dataset):
    def __init__(self, file_path):
        # 读取文件，每行为一个样本，第一个字段为标签，第二个字段为文本
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
        with open('class.txt', 'r', encoding='utf-8') as class_file:
            class_list = class_file.read().splitlines()
        self.labels = []
        self.texts = []
        for line in lines:
            text, label = line.strip().split("\t")
            self.labels.append(class_list.index(label))
            self.texts.append(text)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        label = self.labels[index]
        text = self.texts[index]
        # 对文本进行分词和编码
        encoding = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=128,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)
        return input_ids, attention_mask, label

# 加载训练集和测试集，可以根据你的文件路径修改
train_dataset = TextDataset("IMCS-DAC_train.txt")
test_dataset = TextDataset("IMCS-DAC_test.txt")

# 定义数据加载器
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# 检查是否有可用的GPU设备，并将模型移动到GPU上（在这里加上迁移设备的代码）
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
classifier.to(device) # 将模型移动到GPU上

# 定义训练函数，可以根据需要修改
def train(model, optimizer, criterion, train_loader):
    model.train() # 将模型设置为训练模式
    total_loss = 0 # 记录总的损失值
    total_preds = [] # 记录总的预测值
    total_labels = [] # 记录总的真实标签值
    for batch in train_loader: # 遍历每个批次的数据
        optimizer.zero_grad() # 清空梯度缓存
        input_ids, attention_mask, labels = batch # 取出输入数据和标签数据
        input_ids = input_ids.to(device) # 将输入数据移动到GPU上（如果有）
        attention_mask = attention_mask.to(device) # 将输入数据移动到GPU上（如果有）
        labels = labels.to(device) # 将标签数据移动到GPU上（如果有）
        outputs = model(input_ids, attention_mask) # 调用模型进行前向传播，得到输出结果
        loss = criterion(outputs, labels) # 计算损失值
        loss.backward() # 反向传播计算梯度值
        optimizer.step() # 更新参数值
        total_loss += loss.item() # 累加损失值
        preds = torch.argmax(outputs, dim=1) # 取出预测结果中最大值对应的索引，作为预测类别值
        total_preds.extend(preds.detach().cpu().numpy()) # 将预测值从GPU上移动到CPU上，并转换为numpy数组，然后添加到总的预测值列表中
        total_labels.extend(labels.detach().cpu().numpy()) # 将真实标签值从GPU上移动到CPU上，并转换为numpy数组，然后添加到总的真实标签值列表中
    avg_loss = total_loss / len(train_loader) # 计算平均损失值
    acc_score = accuracy_score(total_labels, total_preds) # 计算准确率分数
    return avg_loss, acc_score

# 定义测试函数，可以根据需要修改
def test(model, criterion, test_loader):
    model.eval() # 将模型设置为评估模式
    total_loss = 0 # 记录总的损失值
    total_preds = [] # 记录总的预测值
    total_labels = [] # 记录总的真实标签值
    
    with torch.no_grad(): # 不计算梯度值，节省内存和时间
        for batch in test_loader: # 遍历每个批次的数据
            input_ids, attention_mask, labels = batch # 取出输入数据和标签数据
            input_ids = input_ids.to(device) # 将输入数据移动到GPU上（如果有）
            attention_mask = attention_mask.to(device) # 将输入数据移动到GPU上（如果有）
            labels = labels.to(device) # 将标签数据移动到GPU上（如果有）
            outputs = model(input_ids, attention_mask) # 调用模型进行前向传播，得到输出结果
            loss = criterion(outputs, labels) # 计算损失值

            total_loss += loss.item() # 累加损失值

            preds = torch.argmax(outputs, dim=1) # 取出预测结果中最大值对应的索引，作为预测类别值
            total_preds.extend(preds.detach().cpu().numpy()) # 将预测值从GPU上移动到CPU上，并转换为numpy数组，然后添加到总的预测值列表中
            total_labels.extend(labels.detach().cpu().numpy()) # 将真实标签值从GPU上移动到CPU上，并转换为numpy数组，然后添加到总的真实标签值列表中

    avg_loss = total_loss / len(test_loader) # 计算平均损失值
    
    acc_score = accuracy_score(total_labels, total_preds) # 计算准确率分数
    
    return avg_loss, acc_score

# 开始训练和测试循环，可以根据需要修改

for epoch in range(num_epochs): # 遍历每个周期
    
    print(f"Epoch {epoch + 1}/{num_epochs}") # 打印当前周期数
    
    train_loss, train_acc_score = train(classifier, optimizer, criterion, train_loader) # 调用训练函数进行训练，并得到训练集上的损失值和准确率分数
    
    print(f"Train loss: {train_loss:.4f}, Train accuracy: {train_acc_score:.4f}")
